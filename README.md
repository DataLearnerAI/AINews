#### 这里我们将主要更新一些重要的AI相关的技术新闻

2023年10月24日：[如何提高大语言模型作为Agent的能力？清华大学与智谱AI推出AgentTuning方案](https://www.datalearner.com/blog/1051698150066829).
尽管开源的大语言模型发展非常迅速，但是，在以大语言模型作为核心的新一代AI Agent解决方案上，开源大语言模型比商业模型表现要明显地差。为了提高大语言模型作为AI Agent的表现和能力，清华大学和智谱AI推出了一种新的方案，AgentTuning，可以将有效增强开源大语言模型作为AI Agent的能力。

2023年9月22日：[截止目前中文领域最大参数量的大模型开源：上海人工智能实验室开源200亿参数的书生·浦语大模型（InternLM 20B系列），性能提升非常明显！](https://www.datalearner.com/blog/1051695354584871)
上海人工智能实验室是国内顶尖的人工智能实验室，此前在大模型领域，他们与商汤科技发布的书生·浦语系列在国内引起了很大的关注。此次，他们又开源了一个全新的200亿参数规模的大语言模型InternLM 20B，应该是截止目前中文领域开源的参数规模最大的一个大模型了。

2023年9月19日：[text-davinci-003后继者！OpenAI发布了一个新的补全大模型：GPT-3.5-Turbo-Instruct，完全的指令模型，没有聊天优化](https://www.datalearner.com/blog/1051695088184015).
OpenAI最新发布了GPT-3.5-Turbo-Instruct，这是一款强大的指令遵循大模型。尽管官方没有发布官方博客介绍，但我们将在本文中详细探讨这一模型的特点以及其在人工智能领域的价值。

2023年9月17日：[LangChain提升大模型基于外部知识检索的准确率的新思路：更改传统文档排序方法，用 LongContextReorder提升大模型回答准确性！](https://www.datalearner.com/blog/1051694962004895).
检索增强生成（Retrieval-augmented generation，RAG）是一种将外部知识检索与大型语言模型生成相结合的方法，通常用于问答系统。当前使用大模型基于外部知识检索结果进行问答是当前大模型与外部知识结合最典型的方式，也是检索增强生成最新的应用。然而，近期的研究表明，这种方式并不总是最佳选择，特别是当检索到的文档数量较多时，这种方式很容易出现回答不准确的情况。为此，LangChain最新推出了LongContextReorder，推出了一种新思路解决这个问题。

2023年9月10日：[TensorRT-LLM：英伟达推出的专为提升大模型推理速度优化的全新框架](https://www.datalearner.com/blog/1051694310279358)
随着大型语言模型（LLM）如 GPT-3 和 BERT 在 AI 领域的崛起，如何在实际应用中高效地进行模型推断成为了一个关键问题。为此，英伟达推出了全新的大模型推理提速框架TensorRT-LM，可以将现有的大模型推理速度提升4倍！
